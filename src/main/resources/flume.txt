## Flume
- Inny przykład narzędzia do strumieniowania
- Zbudowany z myślą o Hadoop (Kafka jest ogólnego przeznaczenis)
- Wspiera HDFS i HBase
- Oryginalnie przeznaczony do obsługi logów z wielu serwerów
- Opiera się o (Source, Channel, Sink) + interceptors
- Wiele wbudowanych typów źródeł np. Kafka, Http, obserwowanie katalogu, elastic search

Przykład oparty o netcat
cd /usr/hdp/current/flume-server/bin
flume-ng agent --conf conf --conf-file ~/example.conf --name a1 -Dflume.root.logger=INFO,console
telnet localhost 44444

Przykład z obserwacją katalogu
mkdir spool
utworzyć katalog w /user/maria_dev/flum w Ambari
cd /usr/hdp/current/flume-server/bin
flume-ng agent --conf conf --conf-file ~/flumelogs.conf --name a1 -Dflume.root.logger=INFO,console

Obsługa z uzyciem Spark Streaming
- Analiza danych w czasie rzeczywistym
- Dane trafiają do recivers, przerabiają na obiekty RDD a od tego momentu obrabiamy juz standardoweo
- Operacje wykonywane na strumieniu aplikowane są na wszystkie obiekty np, map, filter, flatmap
- Mozliwe jest agregowanie danych z jakiegoś czasu (window interval), powtarzanie

mkdir checkpoint
export SPARK_MAJOR_VERSION=2
spark-submit --packages org.apache.spark:spark-streaming-flume_2.11:2.0.0 SparkFlume.py
cd /usr/hdp/current/flume-server/bin
flume-ng agent --conf conf --conf-file ~/sparkstramingflume.conf --name a1
-dodać coś do katalogu spool